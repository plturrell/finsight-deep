#!/usr/bin/env python3
"""Deploy AIQToolkit Distributed System directly to NVIDIA Cloud"""

import os
import json
import asyncio
import urllib.request
import urllib.error
import ssl
from typing import Dict, List, Any
import datetime

# Create SSL context
ssl_context = ssl.create_default_context()

class NVIDIACloudDeployment:
    """Deploy distributed system using NVIDIA cloud APIs"""
    
    def __init__(self):
        self.api_key = "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"
        self.base_url = "https://integrate.api.nvidia.com/v1"
        self.model = "meta/llama-3.1-8b-instruct"
        self.workers = []
        self.tasks = []
        
    def api_call(self, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """Make API call to NVIDIA"""
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.model,
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': max_tokens,
            'temperature': 0.7
        }
        
        request = urllib.request.Request(
            f"{self.base_url}/chat/completions",
            data=json.dumps(data).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        
        try:
            with urllib.request.urlopen(request, context=ssl_context, timeout=30) as response:
                return json.loads(response.read().decode('utf-8'))
        except Exception as e:
            print(f"API Error: {e}")
            return None
    
    def deploy_manager(self):
        """Deploy distributed manager"""
        print("ğŸš€ Deploying Distributed Manager...")
        
        # Initialize manager configuration
        self.manager_config = {
            'id': 'manager-001',
            'host': 'cloud.nvidia.com',
            'port': 50051,
            'status': 'active',
            'workers': [],
            'tasks_queue': [],
            'metrics': {
                'total_tasks': 0,
                'completed_tasks': 0,
                'active_workers': 0
            }
        }
        
        print(f"âœ… Manager deployed: {self.manager_config['id']}")
        print(f"   Endpoint: {self.manager_config['host']}:{self.manager_config['port']}")
        
    def deploy_workers(self, count: int = 4):
        """Deploy distributed workers"""
        print(f"\nğŸš€ Deploying {count} Workers...")
        
        for i in range(count):
            worker = {
                'id': f'worker-{i+1:03d}',
                'gpu_id': i,
                'status': 'ready',
                'model': self.model,
                'tasks_completed': 0,
                'current_task': None
            }
            self.workers.append(worker)
            self.manager_config['workers'].append(worker['id'])
            print(f"âœ… Worker {i+1} deployed: {worker['id']} (GPU {i})")
        
        self.manager_config['metrics']['active_workers'] = count
        
    async def process_task(self, task: Dict[str, Any], worker_id: str) -> Dict[str, Any]:
        """Process a task on a worker"""
        worker = next((w for w in self.workers if w['id'] == worker_id), None)
        if not worker:
            return {'error': 'Worker not found'}
        
        # Update worker status
        worker['status'] = 'busy'
        worker['current_task'] = task['id']
        
        # Simulate processing with real API call
        start_time = datetime.datetime.now()
        
        result = self.api_call(task['prompt'], task.get('max_tokens', 100))
        
        end_time = datetime.datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Update worker status
        worker['status'] = 'ready'
        worker['current_task'] = None
        worker['tasks_completed'] += 1
        
        # Update manager metrics
        self.manager_config['metrics']['completed_tasks'] += 1
        
        return {
            'task_id': task['id'],
            'worker_id': worker_id,
            'result': result['choices'][0]['message']['content'] if result else 'Error',
            'duration': duration,
            'status': 'completed'
        }
    
    async def distribute_tasks(self, tasks: List[Dict[str, Any]]):
        """Distribute tasks across workers"""
        print(f"\nğŸ“Š Distributing {len(tasks)} tasks...")
        
        results = []
        task_queue = tasks.copy()
        
        while task_queue or any(w['status'] == 'busy' for w in self.workers):
            # Assign tasks to available workers
            for worker in self.workers:
                if worker['status'] == 'ready' and task_queue:
                    task = task_queue.pop(0)
                    print(f"ğŸ“‹ Assigning task {task['id']} to {worker['id']}")
                    
                    # Process task asynchronously
                    result = await self.process_task(task, worker['id'])
                    results.append(result)
                    
                    print(f"âœ… Task {task['id']} completed in {result['duration']:.2f}s")
            
            # Small delay to prevent busy loop
            await asyncio.sleep(0.1)
        
        return results
    
    def deploy_monitoring(self):
        """Deploy monitoring dashboard"""
        print("\nğŸ“Š Deploying Monitoring Dashboard...")
        
        monitoring = {
            'grafana': 'http://localhost:3001',
            'prometheus': 'http://localhost:9090',
            'dashboard': 'http://localhost:8080'
        }
        
        print("âœ… Monitoring deployed:")
        for service, url in monitoring.items():
            print(f"   {service}: {url}")
        
        return monitoring
    
    async def run_example_inference(self):
        """Run example distributed inference"""
        print("\nğŸ§ª Running Example Distributed Inference...")
        
        # Create test tasks
        test_prompts = [
            "What is distributed computing?",
            "Explain GPU acceleration",
            "How do neural networks work?",
            "What is machine learning?",
            "Describe cloud computing",
            "What is artificial intelligence?"
        ]
        
        tasks = []
        for i, prompt in enumerate(test_prompts):
            tasks.append({
                'id': f'task-{i+1:03d}',
                'prompt': prompt,
                'max_tokens': 50
            })
        
        # Distribute and process tasks
        results = await self.distribute_tasks(tasks)
        
        # Display results
        print("\nğŸ“Š Results Summary:")
        total_time = sum(r['duration'] for r in results)
        avg_time = total_time / len(results)
        
        print(f"âœ… Processed {len(results)} tasks")
        print(f"â±ï¸  Total time: {total_time:.2f}s")
        print(f"âš¡ Average time: {avg_time:.2f}s per task")
        print(f"ğŸ‘· Workers used: {len(self.workers)}")
        
        # Show sample results
        print("\nğŸ“ Sample Results:")
        for result in results[:3]:
            print(f"\nTask: {result['task_id']}")
            print(f"Worker: {result['worker_id']}")
            print(f"Response: {result['result'][:100]}...")
        
    async def deploy_full_system(self):
        """Deploy complete distributed system"""
        print("ğŸš€ NVIDIA Cloud Deployment Starting...")
        print("=" * 50)
        
        # Deploy components
        self.deploy_manager()
        self.deploy_workers(count=4)
        monitoring = self.deploy_monitoring()
        
        # Show deployment summary
        print("\nğŸ“‹ Deployment Summary")
        print("=" * 50)
        print(f"âœ… Manager: {self.manager_config['id']}")
        print(f"âœ… Workers: {len(self.workers)}")
        print(f"âœ… Model: {self.model}")
        print(f"âœ… API: NVIDIA Cloud")
        
        # Run example
        await self.run_example_inference()
        
        # Final status
        print("\nğŸ‰ Deployment Complete!")
        print("=" * 50)
        print("\nAccess Points:")
        print(f"  API Endpoint: {self.base_url}")
        print(f"  Manager: {self.manager_config['host']}:{self.manager_config['port']}")
        for service, url in monitoring.items():
            print(f"  {service.capitalize()}: {url}")
        
        print("\nTo run more inference:")
        print("  python examples/distributed/run_distributed_inference.py")

async def main():
    """Main deployment function"""
    deployer = NVIDIACloudDeployment()
    await deployer.deploy_full_system()

if __name__ == "__main__":
    print("AIQToolkit - NVIDIA Cloud Deployment")
    print("=" * 50)
    asyncio.run(main())